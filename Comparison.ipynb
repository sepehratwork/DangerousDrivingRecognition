{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHupoYBeS53P"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNNqgKBTbJil"
      },
      "source": [
        "### Dangerous Drving Detection Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9bPpNVcLKjs",
        "outputId": "c7f506f4-a35e-41a8-b906-1b5d06b3e6f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DangerousDrivingRecognition\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/DangerousDrivingRecognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT06ltRIPIL6",
        "outputId": "248907c1-2cdc-4cc6-a1d2-611ddcb1c212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[K     |████████████████████████████████| 238 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (0.1.7)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.21.6)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.15.0)\n",
            "Installing collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.7.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_model_optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLrw5FFsNX5A"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from imutils import paths\n",
        "\n",
        "from imutils import paths\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os\n",
        "from time import time\n",
        "import shutil\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTQKV5dhNaIN"
      },
      "outputs": [],
      "source": [
        "# Open the .txt file which have names of training videos\n",
        "f = open(\"/content/drive/MyDrive/DangerousDrivingRecognition/traintestlist/trainlist.txt\", \"r\")\n",
        "temp = f.read()\n",
        "videos = temp.split('\\n')\n",
        "\n",
        "# Create a dataframe having video names\n",
        "train = pd.DataFrame()\n",
        "train['video_name'] = videos\n",
        "train = train[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h08Wgd31N7OW"
      },
      "outputs": [],
      "source": [
        "# Open the .txt file which have names of test videos\n",
        "with open(\"/content/drive/MyDrive/DangerousDrivingRecognition/traintestlist/testlist.txt\", \"r\") as f:\n",
        "    temp = f.read()\n",
        "videos = temp.split(\"\\n\")\n",
        "\n",
        "# Create a dataframe having video names\n",
        "test = pd.DataFrame()\n",
        "test[\"video_name\"] = videos\n",
        "test = test[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9Z4kKC9N9nE"
      },
      "outputs": [],
      "source": [
        "def extract_tag(video_path):\n",
        "    return video_path.split(\"/\")[1].split(\"_\")[0]\n",
        "\n",
        "def separate_video_name(video_name):\n",
        "    return video_name.split(\"/\")[1]\n",
        "\n",
        "def rectify_video_name(video_name):\n",
        "    return video_name.split(\" \")[0]\n",
        "\n",
        "def move_videos(df, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "    for i in tqdm(range(df.shape[0])):\n",
        "        videoFile = df['video_name'][i].split(\"/\")[-1]\n",
        "        videoPath = os.path.join(\"data\", videoFile)\n",
        "        shutil.copy2(videoPath, output_dir)\n",
        "    print()\n",
        "    print(f\"Total videos: {len(os.listdir(output_dir))}\")\n",
        "\n",
        "train[\"tag\"] = train[\"video_name\"].apply(extract_tag)\n",
        "train[\"video_name\"] = train[\"video_name\"].apply(separate_video_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VtVaaqmOAA8"
      },
      "outputs": [],
      "source": [
        "train[\"video_name\"] = train[\"video_name\"].apply(rectify_video_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGuC08B7OCTe"
      },
      "outputs": [],
      "source": [
        "test[\"tag\"] = test[\"video_name\"].apply(extract_tag)\n",
        "test[\"video_name\"] = test[\"video_name\"].apply(separate_video_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3lx5-HJOEKP"
      },
      "outputs": [],
      "source": [
        "n = 2\n",
        "topNActs = train[\"tag\"].value_counts().nlargest(n).reset_index()[\"index\"].tolist()\n",
        "train_new = train[train[\"tag\"].isin(topNActs)]\n",
        "test_new = test[test[\"tag\"].isin(topNActs)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqKfnA7kOHI1"
      },
      "outputs": [],
      "source": [
        "train_new = train_new.reset_index(drop=True)\n",
        "test_new = test_new.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeRU-r20OJRO"
      },
      "outputs": [],
      "source": [
        "train_new.to_csv(\"train.csv\", index=False)\n",
        "test_new.to_csv(\"test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNU-BkuWOMKk",
        "outputId": "b10abc33-4a4c-4817-b8c5-9b5801d60621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total videos for training: 472\n",
            "Total videos for testing: 80\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xVEYUAubSPt"
      },
      "source": [
        "### UCF101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j8hMztOybU-6"
      },
      "outputs": [],
      "source": [
        "n = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dngpVzlqbpbE"
      },
      "outputs": [],
      "source": [
        "!cp -r -n /content/drive/MyDrive/DangerousDrivingRecognition/UCF101/data ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DamWs9peDTy",
        "outputId": "dcd2db54-823e-4ce9-d52e-96adc0b23cdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[K     |████████████████████████████████| 238 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (1.21.6)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_optimization) (0.1.7)\n",
            "Installing collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.7.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_model_optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gJFMAXTgbscb"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from imutils import paths\n",
        "\n",
        "from imutils import paths\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os\n",
        "from time import time\n",
        "import shutil\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RD7Bdf05bupr",
        "outputId": "2a71f207-5eca-43d0-8327-01b0d527608c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3d610b86-6e77-40a4-ab90-055d3de104f7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d610b86-6e77-40a4-ab90-055d3de104f7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d610b86-6e77-40a4-ab90-055d3de104f7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d610b86-6e77-40a4-ab90-055d3de104f7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                      video_name\n",
              "0  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1\n",
              "1  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1\n",
              "2  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1\n",
              "3  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1\n",
              "4  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Open the .txt file which have names of training videos\n",
        "f = open(\"drive/MyDrive/DangerousDrivingRecognition/UCF101/ucfTrainTestlist/trainlist01.txt\", \"r\")\n",
        "temp = f.read()\n",
        "videos = temp.split('\\n')\n",
        "\n",
        "# Create a dataframe having video names\n",
        "train = pd.DataFrame()\n",
        "train['video_name'] = videos\n",
        "train = train[:-1]\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "G_IwU0zJbxP-",
        "outputId": "30610ee4-1f06-4405-8fec-d22f8fc1c726"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-29dacd1b-fb65-4c7b-b6db-7bc4e2305d3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29dacd1b-fb65-4c7b-b6db-7bc4e2305d3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-29dacd1b-fb65-4c7b-b6db-7bc4e2305d3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-29dacd1b-fb65-4c7b-b6db-7bc4e2305d3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                    video_name\n",
              "0  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\n",
              "1  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi\n",
              "2  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi\n",
              "3  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi\n",
              "4  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Open the .txt file which have names of test videos\n",
        "with open(\"drive/MyDrive/DangerousDrivingRecognition/UCF101/ucfTrainTestlist/testlist01.txt\", \"r\") as f:\n",
        "    temp = f.read()\n",
        "videos = temp.split(\"\\n\")\n",
        "\n",
        "# Create a dataframe having video names\n",
        "test = pd.DataFrame()\n",
        "test[\"video_name\"] = videos\n",
        "test = test[:-1]\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ed0vJTUTb0dj"
      },
      "outputs": [],
      "source": [
        "def extract_tag(video_path):\n",
        "    return video_path.split(\"/\")[0]\n",
        "\n",
        "def separate_video_name(video_name):\n",
        "    return video_name.split(\"/\")[1]\n",
        "\n",
        "def rectify_video_name(video_name):\n",
        "    return video_name.split(\" \")[0]\n",
        "\n",
        "def move_videos(df, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "    for i in tqdm(range(df.shape[0])):\n",
        "        videoFile = df['video_name'][i].split(\"/\")[-1]\n",
        "        videoPath = os.path.join(\"data\", videoFile)\n",
        "        shutil.copy2(videoPath, output_dir)\n",
        "    print()\n",
        "    print(f\"Total videos: {len(os.listdir(output_dir))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7hDG-VuCb2_R",
        "outputId": "06210e51-a5e4-4f19-a7c4-869801430511"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c1ebf3cf-e627-4015-97ff-ff67d48901b1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v_ApplyEyeMakeup_g08_c01.avi 1</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v_ApplyEyeMakeup_g08_c02.avi 1</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v_ApplyEyeMakeup_g08_c03.avi 1</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v_ApplyEyeMakeup_g08_c04.avi 1</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v_ApplyEyeMakeup_g08_c05.avi 1</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1ebf3cf-e627-4015-97ff-ff67d48901b1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1ebf3cf-e627-4015-97ff-ff67d48901b1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1ebf3cf-e627-4015-97ff-ff67d48901b1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                       video_name             tag\n",
              "0  v_ApplyEyeMakeup_g08_c01.avi 1  ApplyEyeMakeup\n",
              "1  v_ApplyEyeMakeup_g08_c02.avi 1  ApplyEyeMakeup\n",
              "2  v_ApplyEyeMakeup_g08_c03.avi 1  ApplyEyeMakeup\n",
              "3  v_ApplyEyeMakeup_g08_c04.avi 1  ApplyEyeMakeup\n",
              "4  v_ApplyEyeMakeup_g08_c05.avi 1  ApplyEyeMakeup"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train[\"tag\"] = train[\"video_name\"].apply(extract_tag)\n",
        "train[\"video_name\"] = train[\"video_name\"].apply(separate_video_name)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "s8J9qAd6b6R3",
        "outputId": "b0356b8b-cba2-4d2c-c29d-5f6fdff381f8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c46a7e28-ec40-433d-b0c5-3314181d698d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v_ApplyEyeMakeup_g08_c01.avi</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v_ApplyEyeMakeup_g08_c02.avi</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v_ApplyEyeMakeup_g08_c03.avi</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v_ApplyEyeMakeup_g08_c04.avi</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v_ApplyEyeMakeup_g08_c05.avi</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c46a7e28-ec40-433d-b0c5-3314181d698d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c46a7e28-ec40-433d-b0c5-3314181d698d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c46a7e28-ec40-433d-b0c5-3314181d698d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                     video_name             tag\n",
              "0  v_ApplyEyeMakeup_g08_c01.avi  ApplyEyeMakeup\n",
              "1  v_ApplyEyeMakeup_g08_c02.avi  ApplyEyeMakeup\n",
              "2  v_ApplyEyeMakeup_g08_c03.avi  ApplyEyeMakeup\n",
              "3  v_ApplyEyeMakeup_g08_c04.avi  ApplyEyeMakeup\n",
              "4  v_ApplyEyeMakeup_g08_c05.avi  ApplyEyeMakeup"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train[\"video_name\"] = train[\"video_name\"].apply(rectify_video_name)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gS64g5z0b7nf",
        "outputId": "1d9503fb-f6b7-41d1-f194-a5cf6062e6c2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ac397b7c-06f9-4aa3-a42f-c94ce3035a0f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_name</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v_ApplyEyeMakeup_g01_c01.avi</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v_ApplyEyeMakeup_g01_c02.avi</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v_ApplyEyeMakeup_g01_c03.avi</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v_ApplyEyeMakeup_g01_c04.avi</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v_ApplyEyeMakeup_g01_c05.avi</td>\n",
              "      <td>ApplyEyeMakeup</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac397b7c-06f9-4aa3-a42f-c94ce3035a0f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac397b7c-06f9-4aa3-a42f-c94ce3035a0f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac397b7c-06f9-4aa3-a42f-c94ce3035a0f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                     video_name             tag\n",
              "0  v_ApplyEyeMakeup_g01_c01.avi  ApplyEyeMakeup\n",
              "1  v_ApplyEyeMakeup_g01_c02.avi  ApplyEyeMakeup\n",
              "2  v_ApplyEyeMakeup_g01_c03.avi  ApplyEyeMakeup\n",
              "3  v_ApplyEyeMakeup_g01_c04.avi  ApplyEyeMakeup\n",
              "4  v_ApplyEyeMakeup_g01_c05.avi  ApplyEyeMakeup"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test[\"tag\"] = test[\"video_name\"].apply(extract_tag)\n",
        "test[\"video_name\"] = test[\"video_name\"].apply(separate_video_name)\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB_Qr-AZb97N",
        "outputId": "90b6dcce-4a28-46a8-ae27-f8e2dbdb8afc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((241, 2), (83, 2))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topNActs = train[\"tag\"].value_counts().nlargest(n).reset_index()[\"index\"].tolist()\n",
        "train_new = train[train[\"tag\"].isin(topNActs)]\n",
        "test_new = test[test[\"tag\"].isin(topNActs)]\n",
        "train_new.shape, test_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ImhO-FjAcCzb"
      },
      "outputs": [],
      "source": [
        "train_new = train_new.reset_index(drop=True)\n",
        "test_new = test_new.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVXCsRFWcGXA",
        "outputId": "cded74cd-a760-477d-f94b-35774aa301f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 241/241 [00:02<00:00, 111.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total videos: 241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 83/83 [00:00<00:00, 205.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total videos: 83\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "move_videos(train_new, f\"train{n}\")\n",
        "move_videos(test_new, f\"test{n}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jPjuxXx1cGUd"
      },
      "outputs": [],
      "source": [
        "train_new.to_csv(f\"train{n}.csv\", index=False)\n",
        "test_new.to_csv(f\"test{n}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjyfSEZheMsx",
        "outputId": "b7e31a3e-064c-4028-b312-61327615342f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total videos for training: 241\n",
            "Total videos for testing: 83\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv(f\"train{n}.csv\")\n",
        "test_df = pd.read_csv(f\"test{n}.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8fcNHtoQTyS"
      },
      "source": [
        "### Kinetics 700-2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzhdf3Y8QVIQ"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/deepmind-media/Datasets/kinetics700_2020.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCHX54220PfE"
      },
      "source": [
        "# **Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7I-iMscdOnCF"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 160\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 500\n",
        "\n",
        "MAX_SEQ_LENGTH = 40\n",
        "NUM_FEATURES = 1056"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "459xFWMxQQbE"
      },
      "source": [
        "# **Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOcl1ae45MGv",
        "outputId": "a29f21f5-f836-43ff-c2a6-7e3bbfd099e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "VGG19 = tf.keras.applications.VGG19(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    pooling=\"avg\",\n",
        "    classes=n,\n",
        "    classifier_activation=\"softmax\",\n",
        ")\n",
        "\n",
        "def build_feature_extractor(conv_model):\n",
        "    \n",
        "    feature_extractor = conv_model\n",
        "\n",
        "    preprocess_input = tf.keras.applications.mobilenet.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"VGG16\")\n",
        "\n",
        "\n",
        "VGG19 = build_feature_extractor(VGG19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1rClPrpOwPT"
      },
      "outputs": [],
      "source": [
        "MobileNet = tf.keras.applications.MobileNet(\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    alpha=1.0,\n",
        "    depth_multiplier=1,\n",
        "    dropout=0.001,\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    pooling=\"avg\",\n",
        "    classes=n,\n",
        "    classifier_activation=\"softmax\",\n",
        ")\n",
        "\n",
        "def build_feature_extractor(conv_model):\n",
        "    \n",
        "    feature_extractor = conv_model\n",
        "\n",
        "    preprocess_input = tf.keras.applications.mobilenet.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"MobileNet\")\n",
        "\n",
        "\n",
        "MobileNet = build_feature_extractor(MobileNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XhbgFOeOyoD"
      },
      "outputs": [],
      "source": [
        "MobileNetV2 = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    alpha=1.0,\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    pooling=\"avg\",\n",
        "    classes=n,\n",
        "    classifier_activation=\"softmax\",\n",
        ")\n",
        "\n",
        "def build_feature_extractor(conv_model):\n",
        "    \n",
        "    feature_extractor = conv_model\n",
        "    \n",
        "    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"MobileNetV2\")\n",
        "\n",
        "\n",
        "MobileNetV2 = build_feature_extractor(MobileNetV2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnM49CuaO0he"
      },
      "outputs": [],
      "source": [
        "MobileNetV3Small = tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    alpha=1.0,\n",
        "    minimalistic=False,\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    classes=n,\n",
        "    pooling=\"avg\",\n",
        "    dropout_rate=0.2,\n",
        "    classifier_activation=\"softmax\",\n",
        "    include_preprocessing=True,\n",
        ")\n",
        "\n",
        "def build_feature_extractor(conv_model):\n",
        "\n",
        "    feature_extractor = conv_model\n",
        "\n",
        "    preprocess_input = tf.keras.applications.mobilenet_v3.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"MobileNetV3Small\")\n",
        "\n",
        "\n",
        "MobileNetV3Small = build_feature_extractor(MobileNetV3Small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfu6ByZlO2Hd"
      },
      "outputs": [],
      "source": [
        "NASNetMobile = tf.keras.applications.NASNetMobile(\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    pooling=\"avg\",\n",
        "    classes=n,\n",
        ")\n",
        "\n",
        "def build_feature_extractor(conv_model):\n",
        "    \n",
        "    feature_extractor = conv_model\n",
        "    \n",
        "    preprocess_input = tf.keras.applications.nasnet.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"NASNetMobile\")\n",
        "\n",
        "\n",
        "NASNetMobile = build_feature_extractor(NASNetMobile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8tMq2MgO4XU"
      },
      "outputs": [],
      "source": [
        "EfficientNetB0 = tf.keras.applications.EfficientNetB0(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    pooling=\"avg\",\n",
        "    classes=n,\n",
        "    classifier_activation=\"softmax\",\n",
        ")\n",
        "\n",
        "def build_feature_extractor(conv_model):\n",
        "    \n",
        "    feature_extractor = conv_model\n",
        "    \n",
        "    preprocess_input = keras.applications.efficientnet.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"EfficientNetB0\")\n",
        "\n",
        "\n",
        "EfficientNetB0 = build_feature_extractor(EfficientNetB0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLqhMwuUO6Y-"
      },
      "outputs": [],
      "source": [
        "EfficientNetV2B0 = tf.keras.applications.EfficientNetV2B0(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    pooling=\"avg\",\n",
        "    classes=n,\n",
        "    classifier_activation=\"softmax\",\n",
        "    include_preprocessing=True,\n",
        ")\n",
        "\n",
        "def build_feature_extractor(conv_model):\n",
        "    \n",
        "    feature_extractor = conv_model\n",
        "    \n",
        "    preprocess_input = keras.applications.efficientnet_v2.preprocess_input\n",
        "    \n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"EfficientNetV2B0\")\n",
        "\n",
        "\n",
        "EfficientNetV2B0 = build_feature_extractor(EfficientNetV2B0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKDNA-yI_4p9"
      },
      "outputs": [],
      "source": [
        "class AccuracyHistory(callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        self.loss = []\n",
        "        self.val_loss = []\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.acc.append(logs.get('acc'))\n",
        "        self.val_acc.append(logs.get('val_acc'))\n",
        "        self.loss.append(logs.get('loss'))\n",
        "        self.val_loss.append(logs.get('val_loss'))\n",
        "\n",
        "history = AccuracyHistory()\n",
        "earlyStopping = callbacks.EarlyStopping(monitor='val_loss', patience=8,min_delta=1e-5, verbose=0, mode='min')\n",
        "mcp_save = callbacks.ModelCheckpoint('mamon98777.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "reduce_lr_loss = callbacks.ReduceLROnPlateau(monitor='val_loss',patience=1, verbose=2,factor=0.5,min_lr=0.0000001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaZr552PPoHK",
        "outputId": "aaf266b1-4886-4f2c-c93f-50612066d54b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 30, 16)            68672     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 8)                 800       \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 8)                 0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 18        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 69,562\n",
            "Trainable params: 69,562\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru (GRU)                   (None, 30, 16)            51552     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 8)                 624       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8)                 0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 18        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 52,266\n",
            "Trainable params: 52,266\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 30, 16)            17168     \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 8)                 200       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8)                 0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 18        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,458\n",
            "Trainable params: 17,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "RNN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH, NUM_FEATURES), name='input'),\n",
        "    tf.keras.layers.SimpleRNN(16, time_major=False, return_sequences=True),\n",
        "    tf.keras.layers.SimpleRNN(8),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(n, activation=\"softmax\")\n",
        "])\n",
        "RNN.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "GRU = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH, NUM_FEATURES), name='input'),\n",
        "    tf.keras.layers.GRU(16, time_major=False, return_sequences=True),\n",
        "    tf.keras.layers.GRU(8),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(n, activation=\"softmax\")\n",
        "])\n",
        "GRU.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "LSTM = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH, NUM_FEATURES), name='input'),\n",
        "    tf.keras.layers.LSTM(16, time_major=False, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(8),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(8, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(n, activation=\"softmax\")\n",
        "])\n",
        "LSTM.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "LSTM2 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH, NUM_FEATURES), name='input'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(160, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(n, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "\n",
        "adam = optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "rms = optimizers.RMSprop()\n",
        "LSTM2.compile(loss='binary_crossentropy', optimizer=adam, metrics=[\"accuracy\"])\n",
        "callbacks=[earlyStopping, mcp_save, reduce_lr_loss,history]\n",
        "\n",
        "LSTM.summary()\n",
        "GRU.summary()\n",
        "RNN.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEyCx9u60fVA"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfy8I4MHOuoV"
      },
      "outputs": [],
      "source": [
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BzGhkyYPOp6"
      },
      "outputs": [],
      "source": [
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIUlUOH3PR4N"
      },
      "outputs": [],
      "source": [
        "def prepare_all_videos(df, root_dir, feature_extractor):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
        "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
        "    # masked with padding or not.\n",
        "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        print(f'{idx}/{len(video_paths)}')\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholders to store the masks and features of the current video.\n",
        "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "\n",
        "                # interpreter = tf.lite.Interpreter(model_path=\"MobileNet.tflite\")\n",
        "                # interpreter.allocate_tensors()\n",
        "                # input_details = interpreter.get_input_details()\n",
        "                # output_details = interpreter.get_output_details()\n",
        "                # input_shape = input_details[0]['shape']\n",
        "                # input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "                # input_data[0] = batch[None, j, :]\n",
        "                # interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "                # interpreter.invoke()\n",
        "                # output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "                # temp_frame_features[i, j, :] = output_data\n",
        "\n",
        "                temp_frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "    return (frame_features, frame_masks), labels\n",
        "\n",
        "\n",
        "train_data, train_labels = prepare_all_videos(train_df, f\"train{n}\", VGG19)\n",
        "test_data, test_labels = prepare_all_videos(test_df, f\"test{n}\", VGG19)\n",
        "\n",
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {train_data[1].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P48pe2_YPqOw"
      },
      "outputs": [],
      "source": [
        "# Utility for running experiments.\n",
        "def run_experiment(seq_model):\n",
        "    filepath = f\"{n}\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    history = seq_model.fit(\n",
        "        [train_data[0]],\n",
        "        train_labels,\n",
        "        validation_split=0.3,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[earlyStopping, mcp_save, reduce_lr_loss,history],\n",
        "        use_multiprocessing=True,\n",
        "    )\n",
        "\n",
        "    seq_model.load_weights(filepath)\n",
        "    _, accuracy = seq_model.evaluate([test_data[0]], test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history, seq_model\n",
        "\n",
        "# print(\"RNN\")\n",
        "# history_rnn, RNN = run_experiment(RNN)\n",
        "# print(\"\\n\")\n",
        "# print(\"GRU\")\n",
        "# history_gru, GRU = run_experiment(GRU)\n",
        "# print(\"\\n\")\n",
        "print(\"LSTM\")\n",
        "history_lstm, LSTM2 = run_experiment(LSTM2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ3Diab9fYvs"
      },
      "source": [
        "## **Saving Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi0pxoEjxVbH"
      },
      "outputs": [],
      "source": [
        "cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxo9vNnuxaFM"
      },
      "outputs": [],
      "source": [
        "MobileNet.save(\"MobileNet.h5\")\n",
        "MobileNetV2.save(\"MobileNetV2.h5\")\n",
        "MobileNetV3Small.save(\"MobileNetV3Small.h5\")\n",
        "NASNetMobile.save(\"NASNetMobile.h5\")\n",
        "EfficientNetB0.save(\"EfficientNetB0.h5\")\n",
        "EfficientNetV2B0.save(\"EfficientNetV2B0.h5\")\n",
        "RNN.save(\"RNN.h5\")\n",
        "GRU.save(\"GRU.h5\")\n",
        "LSTM.save(\"LSTM.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEeJ_EhTSQ69"
      },
      "source": [
        "# **TF Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFfxHi6NHKc3"
      },
      "source": [
        "### Backbones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe_1B5sSHCxx",
        "outputId": "25e0bc80-f3ae-4a80-f129-03365b960a73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "run_model = tf.function(lambda x: MobileNet(x))\n",
        "# This is important, let's fix the input size.\n",
        "BATCH_SIZE = BATCH_SIZE\n",
        "STEPS = MAX_SEQ_LENGTH\n",
        "INPUT_SIZE = NUM_FEATURES\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec([None, 224, 224, 3], MobileNet.inputs[0].dtype))\n",
        "# model directory.\n",
        "MODEL_DIR_MobileNet = \"keras_MobileNet\"\n",
        "MobileNet.save(MODEL_DIR_MobileNet, save_format=\"tf\", signatures=concrete_func)\n",
        "\n",
        "\n",
        "run_model = tf.function(lambda x: MobileNetV2(x))\n",
        "# This is important, let's fix the input size.\n",
        "BATCH_SIZE = BATCH_SIZE\n",
        "STEPS = MAX_SEQ_LENGTH\n",
        "INPUT_SIZE = NUM_FEATURES\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec([None, 224, 224, 3], MobileNetV2.inputs[0].dtype))\n",
        "# model directory.\n",
        "MODEL_DIR_MobileNetV2 = \"keras_MobileNetV2\"\n",
        "MobileNetV2.save(MODEL_DIR_MobileNetV2, save_format=\"tf\", signatures=concrete_func)\n",
        "\n",
        "\n",
        "run_model = tf.function(lambda x: MobileNetV3Small(x))\n",
        "# This is important, let's fix the input size.\n",
        "BATCH_SIZE = BATCH_SIZE\n",
        "STEPS = MAX_SEQ_LENGTH\n",
        "INPUT_SIZE = NUM_FEATURES\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec([None, 224, 224, 3], MobileNetV3Small.inputs[0].dtype))\n",
        "# model directory.\n",
        "MODEL_DIR_MobileNetV3Small = \"keras_MobileNetV3Small\"\n",
        "MobileNetV3Small.save(MODEL_DIR_MobileNetV3Small, save_format=\"tf\", signatures=concrete_func)\n",
        "\n",
        "\n",
        "run_model = tf.function(lambda x: EfficientNetB0(x))\n",
        "# This is important, let's fix the input size.\n",
        "BATCH_SIZE = BATCH_SIZE\n",
        "STEPS = MAX_SEQ_LENGTH\n",
        "INPUT_SIZE = NUM_FEATURES\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec([None, 224, 224, 3], EfficientNetB0.inputs[0].dtype))\n",
        "# model directory.\n",
        "MODEL_DIR_EfficientNetB0 = \"keras_EfficientNetB0\"\n",
        "EfficientNetB0.save(MODEL_DIR_EfficientNetB0, save_format=\"tf\", signatures=concrete_func)\n",
        "\n",
        "\n",
        "run_model = tf.function(lambda x: EfficientNetV2B0(x))\n",
        "# This is important, let's fix the input size.\n",
        "BATCH_SIZE = BATCH_SIZE\n",
        "STEPS = MAX_SEQ_LENGTH\n",
        "INPUT_SIZE = NUM_FEATURES\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec([None, 224, 224, 3], EfficientNetV2B0.inputs[0].dtype))\n",
        "# model directory.\n",
        "MODEL_DIR_EfficientNetV2B0 = \"keras_EfficientNetV2B0\"\n",
        "EfficientNetV2B0.save(MODEL_DIR_EfficientNetV2B0, save_format=\"tf\", signatures=concrete_func)\n",
        "\n",
        "\n",
        "run_model = tf.function(lambda x: NASNetMobile(x))\n",
        "# This is important, let's fix the input size.\n",
        "BATCH_SIZE = BATCH_SIZE\n",
        "STEPS = MAX_SEQ_LENGTH\n",
        "INPUT_SIZE = NUM_FEATURES\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec([None, 224, 224, 3], NASNetMobile.inputs[0].dtype))\n",
        "# model directory.\n",
        "MODEL_DIR_NASNetMobile = \"keras_NASNetMobile\"\n",
        "NASNetMobile.save(MODEL_DIR_NASNetMobile, save_format=\"tf\", signatures=concrete_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruUHs1uDHCQK",
        "outputId": "b287ac85-09ec-476c-c153-69674f66d4f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_MobileNet)\n",
        "tflite_model = converter.convert()\n",
        "with open('MobileNet.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_MobileNetV2)\n",
        "tflite_model = converter.convert()\n",
        "with open('MobileNetV2.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_MobileNetV3Small)\n",
        "tflite_model = converter.convert()\n",
        "with open('MobileNetV3Small.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_EfficientNetB0)\n",
        "tflite_model = converter.convert()\n",
        "with open('EfficientNetB0.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_EfficientNetV2B0)\n",
        "tflite_model = converter.convert()\n",
        "with open('EfficientNetV2B0.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_NASNetMobile)\n",
        "tflite_model = converter.convert()\n",
        "with open('NASNetMobile.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM5ab8UZHCAf",
        "outputId": "5ff05458-d67e-409b-a0c6-043bd64924db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_MobileNet)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.representative_dataset = representative_dataset\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('MobileNet_quantized.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_MobileNetV2)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.representative_dataset = representative_dataset\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('MobileNetV2_quantized.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_MobileNetV3Small)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.representative_dataset = representative_dataset\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('MobileNetV3Small_quantized.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_EfficientNetB0)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.representative_dataset = representative_dataset\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('EfficientNetB0_quantized.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_EfficientNetV2B0)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.representative_dataset = representative_dataset\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('EfficientNetV2B0_quantized.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_NASNetMobile)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.representative_dataset = representative_dataset\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('NASNetMobile_quantized.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQhBNq2rHEDg"
      },
      "source": [
        "### Sequence Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjyHaxpJPyV7",
        "outputId": "91d97144-0461-4ac2-e1dd-be72f73d86e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f78304e84d0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f78304da810> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f78303cf910> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f783039ded0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "source": [
        "run_model = tf.function(lambda x: RNN(x))\n",
        "# This is important, let's fix the input size.\n",
        "BATCH_SIZE = BATCH_SIZE\n",
        "STEPS = MAX_SEQ_LENGTH\n",
        "INPUT_SIZE = NUM_FEATURES\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], RNN.inputs[0].dtype))\n",
        "# model directory.\n",
        "MODEL_DIR_RNN = \"keras_rnn\"\n",
        "RNN.save(MODEL_DIR_RNN, save_format=\"tf\", signatures=concrete_func)\n",
        "\n",
        "\n",
        "run_model = tf.function(lambda x: GRU(x))\n",
        "# This is important, let's fix the input size.\n",
        "BATCH_SIZE = BATCH_SIZE\n",
        "STEPS = MAX_SEQ_LENGTH\n",
        "INPUT_SIZE = NUM_FEATURES\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], GRU.inputs[0].dtype))\n",
        "# model directory.\n",
        "MODEL_DIR_GRU = \"keras_gru\"\n",
        "GRU.save(MODEL_DIR_GRU, save_format=\"tf\", signatures=concrete_func)\n",
        "\n",
        "\n",
        "run_model = tf.function(lambda x: LSTM(x))\n",
        "# This is important, let's fix the input size.\n",
        "BATCH_SIZE = BATCH_SIZE\n",
        "STEPS = MAX_SEQ_LENGTH\n",
        "INPUT_SIZE = NUM_FEATURES\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], LSTM.inputs[0].dtype))\n",
        "# model directory.\n",
        "MODEL_DIR_LSTM = \"keras_lstm\"\n",
        "LSTM.save(MODEL_DIR_LSTM, save_format=\"tf\", signatures=concrete_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00f_YXBpP9Ia",
        "outputId": "c254cfec-0f3e-45cd-d253-2c670cb0bbbf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_RNN)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('RNN.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_GRU)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('GRU.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_LSTM)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('LSTM.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWz7fUJVQACc",
        "outputId": "0635f94e-0a38-481b-856a-948034c2d7e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_RNN)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.representative_dataset = representative_dataset\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('RNN_quantized.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_GRU)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.representative_dataset = representative_dataset\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('GRU_quantized.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR_LSTM)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.representative_dataset = representative_dataset\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('LSTM_quantized.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmE78x2ASbV_"
      },
      "source": [
        "# **Inference Time & Accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXSQE0u0CNWH"
      },
      "source": [
        "### Keras Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7UV2QrgQ4E9"
      },
      "outputs": [],
      "source": [
        "def prepare_single_video(frames, feature_extractor):\n",
        "    frames = frames[None, ...]\n",
        "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "    for i, batch in enumerate(frames):\n",
        "        video_length = batch.shape[0]\n",
        "        length = min(MAX_SEQ_LENGTH, video_length)\n",
        "        for j in range(length):\n",
        "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "    return frame_features, frame_mask\n",
        "\n",
        "\n",
        "def sequence_prediction(path, feature_extractor, sequence_model):\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "    frames = load_video(os.path.join(\"test\", path))\n",
        "    start = time()\n",
        "    frame_features, frame_mask = prepare_single_video(frames, feature_extractor)\n",
        "    probabilities = sequence_model.predict([frame_features, ])[0]\n",
        "    end = time()\n",
        "    inference_time = end - start\n",
        "    return inference_time, frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "QTTDsTzXQ8ro",
        "outputId": "67e62e34-cc87-4a6f-c815-4d34f658ced6"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-a3fec8298f33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mtest_video\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"video_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0minference_time_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMobileNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0minference_time_gru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMobileNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0minference_time_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMobileNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-af525814f240>\u001b[0m in \u001b[0;36msequence_prediction\u001b[0;34m(path, feature_extractor, sequence_model)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mframe_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_single_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-af525814f240>\u001b[0m in \u001b[0;36mprepare_single_video\u001b[0;34m(frames, feature_extractor)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0minput_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\u001b[0m\n\u001b[1;32m    456\u001b[0m           _interpreter_wrapper.CreateWrapperFromFile(\n\u001b[1;32m    457\u001b[0m               \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_resolver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_op_registerers_by_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m               custom_op_registerers_by_func, experimental_preserve_all_tensors))\n\u001b[0m\u001b[1;32m    459\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Failed to open {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not open '<keras.engine.functional.Functional object at 0x7f783bd765d0>.tflite'."
          ]
        }
      ],
      "source": [
        "def with_opencv(filename):\n",
        "    video = cv2.VideoCapture(filename)\n",
        "\n",
        "    duration = video.get(cv2.CAP_PROP_POS_MSEC)\n",
        "    frame_count = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "    return duration, frame_count\n",
        "\n",
        "\n",
        "timing_rnn = 0\n",
        "timing_gru = 0\n",
        "timing_lstm = 0\n",
        "for i in range(20):\n",
        "  test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
        "  inference_time_rnn, test_frames = sequence_prediction(test_video, NASNetMobile, RNN)\n",
        "  inference_time_gru, test_frames = sequence_prediction(test_video, NASNetMobile, GRU)\n",
        "  inference_time_lstm, test_frames = sequence_prediction(test_video, NASNetMobile, LSTM)\n",
        "  filename = f\"/content/drive/MyDrive/DangerousDrivingRecognition/test/{test_video}\"\n",
        "  duration, frame_count = with_opencv(filename)\n",
        "  timing_rnn += ((inference_time_rnn) / frame_count) * 1000\n",
        "  timing_gru += ((inference_time_gru) / frame_count) * 1000\n",
        "  timing_lstm += ((inference_time_lstm) / frame_count) * 1000\n",
        "timing_rnn /= 20\n",
        "timing_gru /= 20\n",
        "timing_lstm /= 20\n",
        "print(f\"\\n\\n\\n\\nRNN Processing of each frame: {timing_rnn}\\n\")\n",
        "print(f\"GRU Processing of each frame: {timing_gru}\\n\")\n",
        "print(f\"LSTM Processing of each frame: {timing_lstm}\\n\\n\")\n",
        "\n",
        "\n",
        "_, accuracy = RNN.evaluate([test_data[0], ], test_labels)\n",
        "print(f\"RNN Test Accuracy: {round(accuracy * 100, 2)}%\\n\")\n",
        "_, accuracy = GRU.evaluate([test_data[0], ], test_labels)\n",
        "print(f\"GRU Test Accuracy: {round(accuracy * 100, 2)}%\\n\")\n",
        "_, accuracy = LSTM.evaluate([test_data[0], ], test_labels)\n",
        "print(f\"LSTM Test Accuracy: {round(accuracy * 100, 2)}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDT_0hfGCR5v"
      },
      "source": [
        "### TF Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VwGZw-XCRGO",
        "outputId": "d84bcb72-f06d-4846-f38c-f00c41a33aff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "RNN Processing of each frame: 30.013563157121343\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def with_opencv(filename):\n",
        "    video = cv2.VideoCapture(filename)\n",
        "\n",
        "    duration = video.get(cv2.CAP_PROP_POS_MSEC)\n",
        "    frame_count = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "    return duration, frame_count\n",
        "\n",
        "\n",
        "inference_time_rnn = 0\n",
        "for i in range(20):\n",
        "  test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
        "\n",
        "  class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "  frames = load_video(os.path.join(\"test\", test_video))\n",
        "  start = time()\n",
        "\n",
        "  frames = frames[None, ...]\n",
        "  frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "\n",
        "  for i, batch in enumerate(frames):\n",
        "      video_length = batch.shape[0]\n",
        "      length = min(MAX_SEQ_LENGTH, video_length)\n",
        "      for j in range(length):\n",
        "          interpreter = tf.lite.Interpreter(model_path=\"MobileNet_quantized.tflite\")\n",
        "          interpreter.allocate_tensors()\n",
        "          input_details = interpreter.get_input_details()\n",
        "          output_details = interpreter.get_output_details()\n",
        "          input_shape = input_details[0]['shape']\n",
        "          input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "          input_data[0] = batch[None, j, :]\n",
        "          interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "          interpreter.invoke()\n",
        "          output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "          frame_features[i, j, :] = output_data\n",
        "\n",
        "\n",
        "  interpreter = tf.lite.Interpreter(model_path=\"RNN_quantized.tflite\")\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  input_shape = input_details[0]['shape']\n",
        "  input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "  input_data[0] = frame_features\n",
        "  interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "  interpreter.invoke()\n",
        "  output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "  probabilities = output_data\n",
        "\n",
        "  end = time()\n",
        "  inference_time_rnn = end - start\n",
        "\n",
        "\n",
        "  filename = f\"/content/drive/MyDrive/DangerousDrivingRecognition/test/{test_video}\"\n",
        "  duration, frame_count = with_opencv(filename)\n",
        "  \n",
        "  inference_time_rnn += ((inference_time_rnn) / frame_count) * 1000\n",
        "\n",
        "\n",
        "inference_time_rnn /= 20\n",
        "\n",
        "print(f\"\\n\\n\\n\\nRNN Processing of each frame: {inference_time_rnn}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yHupoYBeS53P",
        "jCHX54220PfE",
        "459xFWMxQQbE",
        "AEyCx9u60fVA",
        "JJ3Diab9fYvs",
        "EEeJ_EhTSQ69",
        "BFfxHi6NHKc3",
        "FQhBNq2rHEDg",
        "oXSQE0u0CNWH",
        "pDT_0hfGCR5v"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "a71a9e4fb054fff428071484f7faa898cb9ecb31a518fe88e3463da9af879578"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
