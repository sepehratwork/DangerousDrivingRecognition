{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "feature_extractor = load_model('feature_extractor.h5')\n",
    "sequence_model = load_model('sequence_model.h5')\n",
    "IMG_SIZE = 224\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 576\n",
    "\n",
    "\n",
    "start = time()\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "\n",
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    \n",
    "    class_vocab = ['Dangerous', 'Safe']\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    start = time()\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "    end = time()\n",
    "    print(f'Inference Time: {end - start}')\n",
    "    print(f\"{len(frames)} frames\")\n",
    "    inference_time = end - start\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return inference_time, frames\n",
    "\n",
    "\n",
    "test_video = \"/content/Dangerous_1.avi\"\n",
    "print(f\"Test video path: {test_video}\")\n",
    "inference_time, test_frames = sequence_prediction(test_video)\n",
    "# to_gif(test_frames[:90])\n",
    "\n",
    "end = time()\n",
    "\n",
    "duration = end - start\n",
    "print(duration)\n",
    "print(duration/91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "def DangerousDrivingDetection(frames, \n",
    "                                feature_extractor_path='feature_extractor.h5',\n",
    "                                sequence_model_path='sequence_model.h5',\n",
    "                                IMG_SIZE = 224,\n",
    "                                MAX_SEQ_LENGTH = 20,\n",
    "                                NUM_FEATURES = 576):\n",
    "\n",
    "    feature_extractor = load_model(feature_extractor_path)\n",
    "    sequence_model = load_model(sequence_model_path)\n",
    "    IMG_SIZE = IMG_SIZE\n",
    "    MAX_SEQ_LENGTH = MAX_SEQ_LENGTH\n",
    "    NUM_FEATURES = NUM_FEATURES\n",
    "\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    def load_video(frames):\n",
    "        return np.array(frames)\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_single_video(frames):\n",
    "        frames = frames[None, ...]\n",
    "        frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "            frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        return frame_features, frame_mask\n",
    "\n",
    "\n",
    "    def sequence_prediction(frames):\n",
    "        \n",
    "        class_vocab = ['Dangerous', 'Safe']\n",
    "        frames = load_video(frames)\n",
    "        start = time()\n",
    "        frame_features, frame_mask = prepare_single_video(frames)\n",
    "        probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "        end = time()\n",
    "        print(f'Inference Time: {end - start}')\n",
    "        inference_time = end - start\n",
    "\n",
    "        for i in np.argsort(probabilities)[::-1]:\n",
    "            print(f\"{i}  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "        \n",
    "        if probabilities[0] > probabilities[1]:\n",
    "            pass\n",
    "\n",
    "        return inference_time, frames\n",
    "\n",
    "    inference_time, test_frames = sequence_prediction(frames)\n",
    "\n",
    "    end = time()\n",
    "\n",
    "    duration = end - start\n",
    "    print(duration)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(\"Dangerous_10.avi\")\n",
    "\n",
    "counter = 0\n",
    "frames = []\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    counter += 1\n",
    "    IMG_SIZE = 224\n",
    "    frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "    cv2.imshow(\"Camera\", frame)\n",
    "    frame = frame[:, :, [2, 1, 0]]\n",
    "    frames.append(frame)\n",
    "\n",
    "    if counter == 90:\n",
    "        DangerousDrivingDetection(frames=frames,\n",
    "                                    sequence_model_path = \"lstm.h5\")\n",
    "        counter = 0\n",
    "        frames = []\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # if cv2.waitKey(0) == ord(\"q\"):\n",
    "    #     break\n",
    "        # cap.release()\n",
    "        # cv2.destroyAllWindows()\n",
    "    # else:\n",
    "    #     pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(5) as p:\n",
    "        print(p.map(f, [1, 2, 3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "def f(name):\n",
    "    print('hello', name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = Process(target=f, args=('bob',))\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000019FE79C0700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Inference Time: 9.574350357055664\n",
      "0  Dangerous: 84.66%\n",
      "1  Safe: 15.34%\n",
      "9.597288131713867\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32md:\\AI\\3.LearningAlgorithms\\DeepLearning\\Andisheh Negar Pars\\test.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/3.LearningAlgorithms/DeepLearning/Andisheh%20Negar%20Pars/test.ipynb#ch0000004?line=92'>93</a>\u001b[0m counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/3.LearningAlgorithms/DeepLearning/Andisheh%20Negar%20Pars/test.ipynb#ch0000004?line=93'>94</a>\u001b[0m IMG_SIZE \u001b[39m=\u001b[39m \u001b[39m224\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/AI/3.LearningAlgorithms/DeepLearning/Andisheh%20Negar%20Pars/test.ipynb#ch0000004?line=94'>95</a>\u001b[0m frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mresize(frame, (IMG_SIZE, IMG_SIZE))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/3.LearningAlgorithms/DeepLearning/Andisheh%20Negar%20Pars/test.ipynb#ch0000004?line=95'>96</a>\u001b[0m \u001b[39m# frame = cv2.resize(frame,dsize=(224, 224))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AI/3.LearningAlgorithms/DeepLearning/Andisheh%20Negar%20Pars/test.ipynb#ch0000004?line=96'>97</a>\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mCamera\u001b[39m\u001b[39m\"\u001b[39m, frame)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from time import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "def DangerousDrivingDetection(frames, \n",
    "                                feature_extractor_path='feature_extractor.h5',\n",
    "                                sequence_model_path='sequence_model.h5',\n",
    "                                IMG_SIZE = 224,\n",
    "                                MAX_SEQ_LENGTH = 20,\n",
    "                                NUM_FEATURES = 576):\n",
    "\n",
    "    feature_extractor = load_model(feature_extractor_path)\n",
    "    sequence_model = load_model(sequence_model_path)\n",
    "    IMG_SIZE = IMG_SIZE\n",
    "    MAX_SEQ_LENGTH = MAX_SEQ_LENGTH\n",
    "    NUM_FEATURES = NUM_FEATURES\n",
    "\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    def load_video(frames):\n",
    "        return np.array(frames)\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_single_video(frames):\n",
    "        frames = frames[None, ...]\n",
    "        frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"int8\")\n",
    "\n",
    "        interpreter = tf.lite.Interpreter(model_path=\"feature_extractor_quantized.tflite\")\n",
    "        interpreter.allocate_tensors()\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        input_shape = input_details[0]['shape']\n",
    "        \n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                input = np.random.random_sample(input_shape)\n",
    "                input[0] = batch[None, j, :]\n",
    "                # input[0] = cv2.resize((224,224))\n",
    "                input_data = np.array(input, dtype=np.float32)\n",
    "                interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "                \n",
    "                # frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "            frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        return frame_features, frame_mask\n",
    "\n",
    "\n",
    "    def sequence_prediction(frames):\n",
    "        \n",
    "        class_vocab = ['Dangerous', 'Safe']\n",
    "        frames = load_video(frames)\n",
    "        start = time()\n",
    "        frame_features, frame_mask = prepare_single_video(frames)\n",
    "        probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "        end = time()\n",
    "        print(f'Inference Time: {end - start}')\n",
    "        inference_time = end - start\n",
    "\n",
    "        for i in np.argsort(probabilities)[::-1]:\n",
    "            print(f\"{i}  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "        \n",
    "        if probabilities[0] > probabilities[1]:\n",
    "            pass\n",
    "\n",
    "        return inference_time, frames\n",
    "\n",
    "    inference_time, test_frames = sequence_prediction(frames)\n",
    "\n",
    "    end = time()\n",
    "\n",
    "    duration = end - start\n",
    "    print(duration)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(\"Dangerous_10.avi\")\n",
    "\n",
    "counter = 0\n",
    "frames = []\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    counter += 1\n",
    "    IMG_SIZE = 224\n",
    "    frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "    # frame = cv2.resize(frame,dsize=(224, 224))\n",
    "    cv2.imshow(\"Camera\", frame)\n",
    "    frame = frame[:, :, [2, 1, 0]]\n",
    "    frames.append(frame)\n",
    "\n",
    "    if counter == 90:\n",
    "        DangerousDrivingDetection(frames=frames,\n",
    "                                    sequence_model_path = \"lstm.h5\")\n",
    "        counter = 0\n",
    "        frames = []\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # if cv2.waitKey(0) == ord(\"q\"):\n",
    "    #     break\n",
    "        # cap.release()\n",
    "        # cv2.destroyAllWindows()\n",
    "    # else:\n",
    "    #     pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)\n",
    "\n",
    "\n",
    "interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)\n",
    "my_signature = interpreter.get_signature_runner()\n",
    "output = my_signature(x=tf.constant([1.0], shape=(1,10), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"feature_extractor_quantized.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "213524bb45a1aeaf737b1d8c77d7b8db5d425938d9dffc5f4bc6fe6dd3324700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
